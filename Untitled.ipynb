{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     52,
     60,
     64,
     81,
     97,
     113,
     238,
     257,
     271,
     307
    ]
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf8; mode: python -*-\n",
    "\"\"\"\n",
    "Reference implementation of the Tic-Tac-Toe value function learning agent described in Chapter 1 of \"Reinforcement Learning: An Introduction\" by Sutton and Barto.\n",
    "\n",
    "\n",
    "The agent contains a lookup table that maps states to values,\n",
    "where initial values are 1 for a win, 0 for a draw or loss, and 0.5 otherwise.\n",
    "At every move, the agent chooses either the maximum-value move (greedy) or,\n",
    "with some probability epsilon, a random move (exploratory); by default epsilon=0.1.\n",
    "The agent updates its value function (the lookup table) after every greedy move, following the equation:\n",
    "\n",
    "    V(s) <- V(s) + alpha * [ V(s') - V(s) ]\n",
    "\n",
    "This particular implementation addresses the question posed in Exercise 1.1:\n",
    "\n",
    "    What would happen if the RL agent taught itself via self-play?\n",
    "\n",
    "The result is that the agent learns only how to maximize its own potential payoff, without consideration\n",
    "for whether it is playing to a win or a draw. Even more to the point, the agent learns a myopic strategy\n",
    "where it basically has a single path that it wants to take to reach a winning state. If the path is blocked\n",
    "by the opponent, the values will then usually all become 0.5 and the player is effectively moving randomly.\n",
    "\n",
    "\n",
    "## License\n",
    "- Created by Wesley Tansey, 1/21/2013\n",
    "- Online: https://github.com/Naereen/Wesley-Tansey-RL-TicTacToe\n",
    "- Code released under the [MIT license](http://mit-license.org).\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function  # Python 2/3 compatibility !\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# States as integer : manual coding\n",
    "EMPTY = 0\n",
    "PLAYER_X = 1\n",
    "PLAYER_O = 2\n",
    "DRAW = 3\n",
    "\n",
    "BOARD_FORMAT = \"\"\"----------------------------\n",
    "| {0} | {1} | {2} |\n",
    "|--------------------------|\n",
    "| {3} | {4} | {5} |\n",
    "|--------------------------|\n",
    "| {6} | {7} | {8} |\n",
    "----------------------------\"\"\"\n",
    "NAMES = [' ', 'X', 'O']\n",
    "\n",
    "\n",
    "def printboard(state):\n",
    "    \"\"\" Print the board from the internal state.\"\"\"\n",
    "    cells = []\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            cells.append(NAMES[state[i][j]].center(6))\n",
    "    print(BOARD_FORMAT.format(*cells))\n",
    "\n",
    "def emptystate():\n",
    "    \"\"\" An empty 3x3 state.\"\"\"\n",
    "    return [[EMPTY, EMPTY, EMPTY], [EMPTY, EMPTY, EMPTY], [EMPTY, EMPTY, EMPTY]]\n",
    "\n",
    "def gameover(state):\n",
    "    \"\"\" Check if the state is gameover or not.\"\"\"\n",
    "    for i in range(3):\n",
    "        if state[i][0] != EMPTY and state[i][0] == state[i][1] and state[i][0] == state[i][2]:\n",
    "            return state[i][0]\n",
    "        if state[0][i] != EMPTY and state[0][i] == state[1][i] and state[0][i] == state[2][i]:\n",
    "            return state[0][i]\n",
    "    if state[0][0] != EMPTY and state[0][0] == state[1][1] and state[0][0] == state[2][2]:\n",
    "        return state[0][0]\n",
    "    if state[0][2] != EMPTY and state[0][2] == state[1][1] and state[0][2] == state[2][0]:\n",
    "        return state[0][2]\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state[i][j] == EMPTY:\n",
    "                return EMPTY\n",
    "    return DRAW\n",
    "\n",
    "def last_to_act(state):\n",
    "    \"\"\" Count who should play.\"\"\"\n",
    "    countx = 0\n",
    "    counto = 0\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if state[i][j] == PLAYER_X:\n",
    "                countx += 1\n",
    "            elif state[i][j] == PLAYER_O:\n",
    "                counto += 1\n",
    "    if countx == counto:\n",
    "        return PLAYER_O\n",
    "    if countx == (counto + 1):\n",
    "        return PLAYER_X\n",
    "    return -1\n",
    "\n",
    "def enumstates(state, idx, agent):\n",
    "    \"\"\" Enumerate the different states from a state.\"\"\"\n",
    "    if idx > 8:\n",
    "        player = last_to_act(state)\n",
    "        if player == agent.player:\n",
    "            agent.add(state)\n",
    "    else:\n",
    "        winner = gameover(state)\n",
    "        if winner != EMPTY:\n",
    "            return\n",
    "        i = idx // 3\n",
    "        j = idx % 3\n",
    "        for val in range(3):\n",
    "            state[i][j] = val\n",
    "            enumstates(state, idx + 1, agent)\n",
    "\n",
    "class Agent(object):\n",
    "    \"\"\" A RL agent abstraction.\"\"\"\n",
    "\n",
    "    def __init__(self, player, verbose=False, lossval=0, learning=True):\n",
    "        \"\"\" Create a RL agent.\"\"\"\n",
    "        self.values = {}\n",
    "        self.player = player\n",
    "        self.verbose = verbose\n",
    "        self.lossval = lossval\n",
    "        self.learning = learning\n",
    "        self.epsilon = 0.1\n",
    "        self.alpha = 0.99\n",
    "        self.prevstate = None\n",
    "        self.prevscore = 0\n",
    "        self.count = 0\n",
    "        enumstates(emptystate(), 0, self)\n",
    "\n",
    "    def episode_over(self, winner):\n",
    "        \"\"\" Backup and reset self.prevstate and self.prevscore.\"\"\"\n",
    "        self.backup(self.winnerval(winner))\n",
    "        self.prevstate = None\n",
    "        self.prevscore = 0\n",
    "\n",
    "    def action(self, state):\n",
    "        \"\"\" Play an action (epsilon-drunk policy between random and greedy).\"\"\"\n",
    "        r = random.random()\n",
    "        if r < self.epsilon:\n",
    "            move = self.random(state)\n",
    "            self.log('>>>>>>> Exploratory action: ' + str(move))\n",
    "        else:\n",
    "            move = self.greedy(state)\n",
    "            self.log('>>>>>>> Best action: ' + str(move))\n",
    "        state[move[0]][move[1]] = self.player\n",
    "        self.prevstate = self.statetuple(state)\n",
    "        self.prevscore = self.lookup(state)\n",
    "        state[move[0]][move[1]] = EMPTY\n",
    "        return move\n",
    "\n",
    "    def random(self, state):\n",
    "        \"\"\" Random policy !\"\"\"\n",
    "        available = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i][j] == EMPTY:\n",
    "                    available.append((i, j))\n",
    "        return random.choice(available)\n",
    "\n",
    "    def greedy(self, state):\n",
    "        \"\"\" Naive implementation of the greedy policy.\"\"\"\n",
    "        maxval = -50000\n",
    "        maxmove = None\n",
    "        if self.verbose:\n",
    "            cells = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if state[i][j] == EMPTY:\n",
    "                    state[i][j] = self.player\n",
    "                    val = self.lookup(state)\n",
    "                    state[i][j] = EMPTY\n",
    "                    if val > maxval:\n",
    "                        maxval = val\n",
    "                        maxmove = (i, j)\n",
    "                    if self.verbose:\n",
    "                        cells.append('{0:.3f}'.format(val).center(6))\n",
    "                elif self.verbose:\n",
    "                    cells.append(NAMES[state[i][j]].center(6))\n",
    "        if self.verbose:\n",
    "            print(BOARD_FORMAT.format(*cells))\n",
    "        self.backup(maxval)\n",
    "        return maxmove\n",
    "\n",
    "    def backup(self, nextval):\n",
    "        \"\"\" Backup the next value.\"\"\"\n",
    "        if self.prevstate is not None and self.learning:\n",
    "            self.values[self.prevstate] += self.alpha * (nextval - self.prevscore)\n",
    "\n",
    "    def lookup(self, state):\n",
    "        \"\"\" Lookup a state.\"\"\"\n",
    "        key = self.statetuple(state)\n",
    "        if key not in self.values:\n",
    "            self.add(key)\n",
    "        return self.values[key]\n",
    "\n",
    "    def add(self, state):\n",
    "        \"\"\" Add a state.\"\"\"\n",
    "        winner = gameover(state)\n",
    "        tup = self.statetuple(state)\n",
    "        self.values[tup] = self.winnerval(winner)\n",
    "\n",
    "    def winnerval(self, winner):\n",
    "        \"\"\" Return the value of the winner (0, .5, 1, or self.lossval).\"\"\"\n",
    "        if winner == self.player:\n",
    "            return 1\n",
    "        elif winner == EMPTY:\n",
    "            return 0.5\n",
    "        elif winner == DRAW:\n",
    "            return 0\n",
    "        else:\n",
    "            return self.lossval\n",
    "\n",
    "    def printvalues(self):\n",
    "        \"\"\" Print the current internal values.\"\"\"\n",
    "        vals = deepcopy(self.values)\n",
    "        for key in vals:\n",
    "            state = [list(key[0]), list(key[1]), list(key[2])]\n",
    "            cells = []\n",
    "            for i in range(3):\n",
    "                for j in range(3):\n",
    "                    if state[i][j] == EMPTY:\n",
    "                        state[i][j] = self.player\n",
    "                        cells.append(str(self.lookup(state)).center(3))\n",
    "                        state[i][j] = EMPTY\n",
    "                    else:\n",
    "                        cells.append(NAMES[state[i][j]].center(3))\n",
    "            print(BOARD_FORMAT.format(*cells))\n",
    "\n",
    "    def statetuple(self, state):\n",
    "        \"\"\" Return a tuple of tuple for the current state.\"\"\"\n",
    "        return (tuple(state[0]), tuple(state[1]), tuple(state[2]))\n",
    "\n",
    "    def log(self, s):\n",
    "        \"\"\" Print if verbose.\"\"\"\n",
    "        if self.verbose:\n",
    "            print(s)\n",
    "\n",
    "class Human(object):\n",
    "    \"\"\" An interactive player. \"\"\"\n",
    "    def __init__(self, player):\n",
    "        \"\"\" Create an interactive player.\"\"\"\n",
    "        self.player = player\n",
    "\n",
    "    def action(self, state):\n",
    "        \"\"\" Ask (with input(...)) the user to play.\"\"\"\n",
    "        printboard(state)\n",
    "        action = str(input('Your move? '))\n",
    "        return (int(action.split(',')[0]), int(action.split(',')[1]))\n",
    "\n",
    "    def episode_over(self, winner):\n",
    "        \"\"\" Check if you win.\"\"\"\n",
    "        if winner == DRAW:\n",
    "            print('Game over! It was a draw.')\n",
    "        else:\n",
    "            print('Game over! Winner: Player {0}'.format(winner))\n",
    "\n",
    "def play(agent1, agent2):\n",
    "    \"\"\" Play once.\"\"\"\n",
    "    state = emptystate()\n",
    "    for i in range(9):\n",
    "        if i % 2 == 0:\n",
    "            move = agent1.action(state)\n",
    "        else:\n",
    "            move = agent2.action(state)\n",
    "        state[move[0]][move[1]] = (i % 2) + 1\n",
    "        winner = gameover(state)\n",
    "        if winner != EMPTY:\n",
    "            return winner\n",
    "    return winner\n",
    "\n",
    "def measure_performance_vs_random(agent1, agent2):\n",
    "    \"\"\" A naive way to measure performance of two agents vs random.\"\"\"\n",
    "    epsilon1 = agent1.epsilon\n",
    "    epsilon2 = agent2.epsilon\n",
    "    agent1.epsilon = 0\n",
    "    agent2.epsilon = 0\n",
    "    agent1.learning = False\n",
    "    agent2.learning = False\n",
    "    r1 = Agent(1)\n",
    "    r2 = Agent(2)\n",
    "    r1.epsilon = 1\n",
    "    r2.epsilon = 1\n",
    "    probs = [0, 0, 0, 0, 0, 0]\n",
    "    games = 100\n",
    "    for i in range(games):\n",
    "        winner = play(agent1, r2)\n",
    "        if winner == PLAYER_X:\n",
    "            probs[0] += 1.0 / games\n",
    "        elif winner == PLAYER_O:\n",
    "            probs[1] += 1.0 / games\n",
    "        else:\n",
    "            probs[2] += 1.0 / games\n",
    "    for i in range(games):\n",
    "        winner = play(r1, agent2)\n",
    "        if winner == PLAYER_O:\n",
    "            probs[3] += 1.0 / games\n",
    "        elif winner == PLAYER_X:\n",
    "            probs[4] += 1.0 / games\n",
    "        else:\n",
    "            probs[5] += 1.0 / games\n",
    "    agent1.epsilon = epsilon1\n",
    "    agent2.epsilon = epsilon2\n",
    "    agent1.learning = True\n",
    "    agent2.learning = True\n",
    "    return probs\n",
    "\n",
    "def measure_performance_vs_each_other(agent1, agent2):\n",
    "    \"\"\" A naive way to measure performance of two agents vs each other.\"\"\"\n",
    "    # epsilon1 = agent1.epsilon\n",
    "    # epsilon2 = agent2.epsilon\n",
    "    # agent1.epsilon = 0\n",
    "    # agent2.epsilon = 0\n",
    "    # agent1.learning = False\n",
    "    # agent2.learning = False\n",
    "    probs = [0, 0, 0]\n",
    "    games = 100\n",
    "    for i in range(games):\n",
    "        winner = play(agent1, agent2)\n",
    "        if winner == PLAYER_X:\n",
    "            probs[0] += 1.0 / games\n",
    "        elif winner == PLAYER_O:\n",
    "            probs[1] += 1.0 / games\n",
    "        else:\n",
    "            probs[2] += 1.0 / games\n",
    "    # agent1.epsilon = epsilon1\n",
    "    # agent2.epsilon = epsilon2\n",
    "    # agent1.learning = True\n",
    "    # agent2.learning = True\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    p1 = Agent(1, lossval=-1)\n",
    "    p2 = Agent(2, lossval=-1)\n",
    "    r1 = Agent(1, learning=False)\n",
    "    r2 = Agent(2, learning=False)\n",
    "    r1.epsilon = 1\n",
    "    r2.epsilon = 1\n",
    "    series = ['P1-Win', 'P1-Lose', 'P1-Draw', 'P2-Win', 'P2-Lose', 'P2-Draw']\n",
    "    # series = ['P1-Win', 'P2-Win', 'Draw']\n",
    "    colors = ['r', 'b', 'g', 'c', 'm', 'b']\n",
    "    markers = ['+', '.', 'o', '*', '^', 's']\n",
    "    f = open('results.csv', 'wb')\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(series)\n",
    "    perf = [[] for _ in range(len(series) + 1)]\n",
    "    for i in range(10000):\n",
    "        if i % 10 == 0:\n",
    "            print('Game: {0}'.format(i))\n",
    "            probs = measure_performance_vs_random(p1, p2)\n",
    "            writer.writerow(probs)\n",
    "            f.flush()\n",
    "            perf[0].append(i)\n",
    "            for idx, x in enumerate(probs):\n",
    "                perf[idx + 1].append(x)\n",
    "        winner = play(p1, p2)\n",
    "        p1.episode_over(winner)\n",
    "        # winner = play(r1, p2)\n",
    "        p2.episode_over(winner)\n",
    "    f.close()\n",
    "    for i in range(1, len(perf)):\n",
    "        plt.plot(perf[0], perf[i], label=series[i - 1], color=colors[i - 1])\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('RL Agent Performance vs. Random Agent\\n({0} loss value, self-play)'.format(p1.lossval))\n",
    "    # plt.title('P1 Loss={0} vs. P2 Loss={1}'.format(p1.lossval, p2.lossval))\n",
    "    plt.legend()\n",
    "    # plt.show()\n",
    "    # plt.savefig('p1loss{0}vsp2loss{1}.png'.format(p1.lossval, p2.lossval))\n",
    "    plt.savefig('selfplay_random_{0}loss.png'.format(p1.lossval))\n",
    "    while True:\n",
    "        p2.verbose = True\n",
    "        p1 = Human(1)\n",
    "        winner = play(p1, p2)\n",
    "        p1.episode_over(winner)\n",
    "        p2.episode_over(winner)\n",
    "\n",
    "# End of tictactoe.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 82f99ac] commit\n",
      " 1 file changed, 192 insertions(+), 2 deletions(-)\n",
      "Counting objects: 3, done.\n",
      "Delta compression using up to 4 threads.\n",
      "Compressing objects: 100% (3/3), done.\n",
      "Writing objects: 100% (3/3), 839 bytes | 839.00 KiB/s, done.\n",
      "Total 3 (delta 1), reused 0 (delta 0)\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To https://github.com/JaeDukSeo/tictactoe.git\n",
      "   81c1f84..82f99ac  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git all-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
